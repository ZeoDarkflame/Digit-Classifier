{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier\n",
    "Using MNIST datasets from http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # to plot the cost\n",
    "import struct # to read the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_one_hot(a):\n",
    "    # function to reverse the 'one hot' created\n",
    "    b = np.zeros((1,a.shape[1]))\n",
    "    for i in range(a.shape[1]):\n",
    "        for j in range(a.shape[0]):\n",
    "            if(a[j,i] == 1):\n",
    "                b[0,i] = j\n",
    "                break\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(layers_info,X):\n",
    "    # initializing parameters\n",
    "    parameters = {}\n",
    "    layers_info.insert(0,X.shape[0])\n",
    "    for i in range(1,len(layers_info)):\n",
    "        parameters['W'+str(i)] = np.random.randn(layers_info[i],layers_info[i-1]) * 0.01\n",
    "        parameters['b'+str(i)] = np.zeros((layers_info[i],1))\n",
    "    return parameters,layers_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(parameters,X,layers_info):\n",
    "    # implementing forward propagation\n",
    "    logits = {}\n",
    "    activations = {}\n",
    "    activations['A0'] = X\n",
    "    for i in range(1,len(layers_info)-1):\n",
    "        logits['Z'+str(i)] = np.dot(parameters['W'+str(i)],activations['A'+str(i-1)]) + parameters['b'+str(i)]\n",
    "        activations['A'+str(i)] = tanh(logits['Z'+str(i)])\n",
    "        \n",
    "    logits['Z'+str(len(layers_info)-1)] = np.dot(parameters['W'+str(len(layers_info)-1)],activations['A'+str(len(layers_info)-1-1)]) + parameters['b'+str(len(layers_info)-1)]\n",
    "    activations['A'+str(len(layers_info)-1)] = np.exp(logits['Z'+str(len(layers_info)-1)])/np.sum(np.exp(logits['Z'+str(len(layers_info)-1)]),axis=0,keepdims=True)\n",
    "    \n",
    "    return logits,activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(activations,Y,layers_info):\n",
    "    # calculating cost\n",
    "    AL = activations['A'+str(len(layers_info)-1)]\n",
    "    cost = (-1/Y.shape[1])*np.sum(Y*np.log(AL))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X,Y,parameters,activations,layers_info,logits):\n",
    "    # implementing backward propagation\n",
    "    gradients = {}\n",
    "    m = Y.shape[1]\n",
    "    gradients['dZ'+str(len(layers_info)-1)] = activations['A'+str(len(layers_info)-1)] - Y\n",
    "    gradients['dW'+str(len(layers_info)-1)] = (1/m) * np.dot(gradients['dZ'+str(len(layers_info)-1)],activations['A'+str(len(layers_info)-2)].T)\n",
    "    gradients['db'+str(len(layers_info)-1)] = (1/m) * np.sum(gradients['dZ'+str(len(layers_info)-1)],axis=1,keepdims=True)\n",
    "    for i in range(2,len(layers_info)):\n",
    "        gradients['dZ'+str(len(layers_info) - i)] = np.dot(parameters['W'+str(len(layers_info)-i+1)].T,gradients['dZ'+str(len(layers_info)-i+1)])*(1-np.square(tanh(logits['Z'+str(len(layers_info)-i)])))\n",
    "        gradients['dW'+str(len(layers_info)-i)] = (1/m) * np.dot(gradients['dZ'+str(len(layers_info)-i)],activations['A'+str(len(layers_info)-i-1)].T)\n",
    "        gradients['db'+str(len(layers_info)-i)] = (1/m) * np.sum(gradients['dZ'+str(len(layers_info)-i)],axis=1,keepdims=True)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(X,Y,layers_info,max_iter,learning_rate=0.01):\n",
    "    # Main loop to start training\n",
    "    \n",
    "    parameters,layers_info = initializer(layers_info,X)\n",
    "    \n",
    "    global costs # this is global so plot can be made even outside the function\n",
    "    costs = []\n",
    "    \n",
    "    for j in range(max_iter):\n",
    "        logits,activations = forward_prop(parameters,X,layers_info)\n",
    "        #for j in logits.keys():\n",
    "        #    print(j)\n",
    "        cost = compute_cost(activations,Y,layers_info)\n",
    "        costs.append(cost)\n",
    "        gradients = back_prop(X,Y,parameters,activations,layers_info,logits)\n",
    "        for i in range(1,len(layers_info)):\n",
    "            parameters['W'+str(i)] -= learning_rate*gradients['dW'+str(i)]\n",
    "            parameters['b'+str(i)] -= learning_rate*gradients['db'+str(i)]\n",
    "            \n",
    "        if(j%100 == 0):\n",
    "            print(cost)\n",
    "            \n",
    "    plt.plot(costs)\n",
    "    #plt.ylim(0,5)\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Num Iterations')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening and manipulating the Dataset\n",
    "\n",
    "with open('datasets/Hdigits_train.idx3-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "    data = data.reshape((size, nrows, ncols))\n",
    "X = data\n",
    "with open('datasets/Hdigits_train_labels.idx1-ubyte','rb') as f:\n",
    "    magic, size = struct.unpack(\">II\", f.read(8))\n",
    "    data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "Y_raw = data\n",
    "X = X/255\n",
    "X = X.reshape(X.shape[0],-1)\n",
    "X = X.T\n",
    "Y_raw = Y_raw.reshape(1,60000)\n",
    "Y_hot = np.zeros((10,60000))\n",
    "for i in range(60000):             #one hotting the labels\n",
    "    Y_hot[Y_raw[0,i],i] = 1\n",
    "Y = Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3002930915178736\n",
      "1.5490620179468133\n",
      "1.1902854356732897\n",
      "0.997025574409552\n"
     ]
    }
   ],
   "source": [
    "layers_info = [10] # define layers here\n",
    "trained_parameters = start_training(X,Y,layers_info,1000) # define max_iterations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
